var documenterSearchIndex = {"docs":
[{"location":"api/#Library-Reference","page":"Library Reference","title":"Library Reference","text":"","category":"section"},{"location":"api/#Systems","page":"Library Reference","title":"Systems","text":"","category":"section"},{"location":"api/","page":"Library Reference","title":"Library Reference","text":"GaussianSystem\nGaussianSystem(::AbstractMatrix, ::AbstractMatrix, ::AbstractVector, ::AbstractVector, ::Any)\n\ncanon(::AbstractMatrix, ::AbstractVector)\ncanon(::AbstractMatrix)\nnormal(::AbstractMatrix, ::AbstractVector)\nnormal(::AbstractMatrix)\nnormal(::AbstractVector)\nkernel(::AbstractMatrix, ::AbstractVector, ::AbstractMatrix)\nkernel(::AbstractMatrix, ::AbstractMatrix)\nkernel(::AbstractMatrix)\n\nlength(::GaussianSystem)\ncov(::GaussianSystem)\ninvcov(::GaussianSystem)\nmean(::GaussianSystem)\n⊗(::GaussianSystem, ::GaussianSystem)\n+(::GaussianSystem, ::GaussianSystem)\n*(::GaussianSystem, ::AbstractMatrix)\nzero(::GaussianSystem)\npushfwd\nmarginal\n\noapply(::UndirectedWiringDiagram, ::AbstractDict{<:Any, <:GaussianSystem})\noapply(::UndirectedWiringDiagram, ::AbstractVector{<:GaussianSystem})","category":"page"},{"location":"api/#AlgebraicInference.GaussianSystem","page":"Library Reference","title":"AlgebraicInference.GaussianSystem","text":"GaussianSystem{\n    T₁ <: AbstractMatrix,\n    T₂ <: AbstractMatrix, \n    T₃ <: AbstractVector,\n    T₄ <: AbstractVector,\n    T₅}\n\nAn n-variate Gaussian system with fiber mathbbL subseteq mathbbR^n is a probability space Sigma = (mathbbR^n mathcalE P) isomorphic to a Gaussian measure on the quotient space mathbbR^n  mathbbL.\n\nGaussian systems correspond to convex energy functions of the form\n\n    E(x) = begincases\n        frac12x^mathsfT P x - x^mathsfTp  Sx = s \n        infty                                         textelse\n    endcases\n\nwhere P and S are positive semidefinite, p in mathttrange(P), and s in mathttrange(S).\n\nIf the fiber of a system Sigma is zero-dimensional, then Sigma is a multivariate normal distribution, and the energy function of Sigma is its negative log-density.\n\nReferences:\n\nJ. C. Willems, \"Open Stochastic Systems,\" in IEEE Transactions on Automatic Control,  vol. 58, no. 2, pp. 406-421, Feb. 2013, doi: 10.1109/TAC.2012.2210836.\n\n\n\n\n\n","category":"type"},{"location":"api/#AlgebraicInference.GaussianSystem-Tuple{AbstractMatrix, AbstractMatrix, AbstractVector, AbstractVector, Any}","page":"Library Reference","title":"AlgebraicInference.GaussianSystem","text":"GaussianSystem(\n    P::AbstractMatrix,\n    S::AbstractMatrix,\n    p::AbstractVector,\n    s::AbstractVector,\n    σ)\n\nConstruct a Gaussian system by specifying its energy function. You should set sigma = s^mathsfT S^+ s, where S^+ is the Moore-Penrose psuedoinverse of S.\n\n\n\n\n\n","category":"method"},{"location":"api/#AlgebraicInference.canon-Tuple{AbstractMatrix, AbstractVector}","page":"Library Reference","title":"AlgebraicInference.canon","text":"canon(J::AbstractMatrix, h::AbstractVector)\n\nConstruct a multivariate normal distribution with information matrix J and potential vector h.\n\n\n\n\n\n","category":"method"},{"location":"api/#AlgebraicInference.canon-Tuple{AbstractMatrix}","page":"Library Reference","title":"AlgebraicInference.canon","text":"canon(J::AbstractMatrix)\n\nConstruct a centered multivariate normal distribution with information matrix J.\n\n\n\n\n\n","category":"method"},{"location":"api/#AlgebraicInference.normal-Tuple{AbstractMatrix, AbstractVector}","page":"Library Reference","title":"AlgebraicInference.normal","text":"normal(Σ::AbstractMatrix, μ::AbstractVector)\n\nConstruct a multivariate normal distribution with covariance matrix Σ and mean vector μ.\n\n\n\n\n\n","category":"method"},{"location":"api/#AlgebraicInference.normal-Tuple{AbstractMatrix}","page":"Library Reference","title":"AlgebraicInference.normal","text":"normal(Σ::AbstractMatrix)\n\nConstruct a centered multivariate normal distribution with covariance matrix Σ.\n\n\n\n\n\n","category":"method"},{"location":"api/#AlgebraicInference.normal-Tuple{AbstractVector}","page":"Library Reference","title":"AlgebraicInference.normal","text":"normal(μ::AbstractVector)\n\nConstruct a Dirac distribution with mean vector μ.\n\n\n\n\n\n","category":"method"},{"location":"api/#AlgebraicInference.kernel-Tuple{AbstractMatrix, AbstractVector, AbstractMatrix}","page":"Library Reference","title":"AlgebraicInference.kernel","text":"kernel(Σ::AbstractMatrix, μ::AbstractVector, L::AbstractMatrix)\n\nConstruct a conditional distribution of the form\n\n    y mid x sim mathcalN(Lx + mu Sigma)\n\n\n\n\n\n","category":"method"},{"location":"api/#AlgebraicInference.kernel-Tuple{AbstractMatrix, AbstractMatrix}","page":"Library Reference","title":"AlgebraicInference.kernel","text":"kernel(Σ::AbstractMatrix, L::AbstractMatrix)\n\nConstruct a conditional distribution of the form\n\n    y mid x sim mathcalN(Lx Sigma)\n\n\n\n\n\n","category":"method"},{"location":"api/#AlgebraicInference.kernel-Tuple{AbstractMatrix}","page":"Library Reference","title":"AlgebraicInference.kernel","text":"kernel(L::AbstractMatrix)\n\nConstruct a conditional distribution of the form\n\n    y mid x sim delta_Lx\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.length-Tuple{GaussianSystem}","page":"Library Reference","title":"Base.length","text":"length(Σ::GaussianSystem)\n\nGet the dimension of Σ.\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.cov-Tuple{GaussianSystem}","page":"Library Reference","title":"Statistics.cov","text":"cov(Σ::GaussianSystem)\n\nGet the covariance matrix of Σ.\n\n\n\n\n\n","category":"method"},{"location":"api/#AlgebraicInference.invcov-Tuple{GaussianSystem}","page":"Library Reference","title":"AlgebraicInference.invcov","text":"invcov(Σ::GaussianSystem)\n\nGet the information matrix of Σ.\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.mean-Tuple{GaussianSystem}","page":"Library Reference","title":"Statistics.mean","text":"mean(Σ::GaussianSystem)\n\nGet the mean vector of Σ.\n\n\n\n\n\n","category":"method"},{"location":"api/#Catlab.Theories.:⊗-Tuple{GaussianSystem, GaussianSystem}","page":"Library Reference","title":"Catlab.Theories.:⊗","text":"⊗(Σ₁::GaussianSystem, Σ₂::GaussianSystem)\n\nCompute the tensor product of Σ₁ and Σ₂.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.:+-Tuple{GaussianSystem, GaussianSystem}","page":"Library Reference","title":"Base.:+","text":"+(Σ₁::GaussianSystem, Σ₂::GaussianSystem)\n\nConstruct a Gaussian system by summing the energy functions of Σ₁ and Σ₂.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.:*-Tuple{GaussianSystem, AbstractMatrix}","page":"Library Reference","title":"Base.:*","text":"*(Σ::GaussianSystem, M::AbstractMatrix)\n\nConstruct a Gaussian system by pulling the energy function of Σ back along M.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.zero-Tuple{GaussianSystem}","page":"Library Reference","title":"Base.zero","text":"zero(Σ::GaussianSystem)\n\nConstruct a Gaussian system with energy function E(x) = 0.\n\n\n\n\n\n","category":"method"},{"location":"api/#AlgebraicInference.pushfwd","page":"Library Reference","title":"AlgebraicInference.pushfwd","text":"pushfwd(M::AbstractMatrix, Σ::GaussianSystem)\n\nCompute the pushforward of Σ along M.\n\n\n\n\n\n","category":"function"},{"location":"api/#AlgebraicInference.marginal","page":"Library Reference","title":"AlgebraicInference.marginal","text":"marginal(m::AbstractVector{Bool}, Σ::GaussianSystem)\n\nCompute the marginal of Σ along the indices specified by m.\n\n\n\n\n\n","category":"function"},{"location":"api/#Catlab.WiringDiagrams.WiringDiagramAlgebras.oapply-Tuple{AbstractUWD, AbstractDict{<:Any, <:GaussianSystem}}","page":"Library Reference","title":"Catlab.WiringDiagrams.WiringDiagramAlgebras.oapply","text":"oapply(wd::UndirectedWiringDiagram, box_map::AbstractDict{<:Any, <:GaussianSystem})\n\nSee oapply(wd::UndirectedWiringDiagram, boxes::AbstractVector{<:GaussianSystem}).\n\n\n\n\n\n","category":"method"},{"location":"api/#Catlab.WiringDiagrams.WiringDiagramAlgebras.oapply-Tuple{AbstractUWD, AbstractVector{<:GaussianSystem}}","page":"Library Reference","title":"Catlab.WiringDiagrams.WiringDiagramAlgebras.oapply","text":"oapply(wd::UndirectedWiringDiagram, boxes::AbstractVector{<:GaussianSystem})\n\nCompose Gaussian systems according to the undirected wiring diagram wd.\n\n\n\n\n\n","category":"method"},{"location":"api/#Valuations","page":"Library Reference","title":"Valuations","text":"","category":"section"},{"location":"api/","page":"Library Reference","title":"Library Reference","text":"Valuation\nIdentityValuation\nLabeledBox\n\ndomain\ncombine\nproject\n\ninference_problem(::UndirectedWiringDiagram, ::AbstractDict)\ninference_problem(::UndirectedWiringDiagram, ::AbstractVector)","category":"page"},{"location":"api/#AlgebraicInference.Valuation","page":"Library Reference","title":"AlgebraicInference.Valuation","text":"Valuation{T}\n\nAbstract type for valuations in a valuation algebra.\n\nSubtypes should specialize the following methods:\n\ndomain(ϕ::Valuation)\ncombine(ϕ₁::Valuation, ϕ₂::Valuation)\nproject(ϕ::Valuation, x)\n\nValuations are parametrized by the type of the variables in their variable system. If isa(ϕ, Valuation{T}), then domain(ϕ) should return a container with element type T.\n\nReferences:\n\nPouly, M.; Kohlas, J. Generic Inference. A Unified Theory for Automated Reasoning; Wiley: Hoboken, NJ, USA, 2011.\n\n\n\n\n\n","category":"type"},{"location":"api/#AlgebraicInference.IdentityValuation","page":"Library Reference","title":"AlgebraicInference.IdentityValuation","text":"IdentityValuation{T} <: Valuation{T}\n\nThe identity element e.\n\n\n\n\n\n","category":"type"},{"location":"api/#AlgebraicInference.LabeledBox","page":"Library Reference","title":"AlgebraicInference.LabeledBox","text":"LabeledBox{T₁, T₂} <: Valuation{T₁}\n\n\n\n\n\n","category":"type"},{"location":"api/#AlgebraicInference.domain","page":"Library Reference","title":"AlgebraicInference.domain","text":"domain(ϕ::Valuation)\n\nGet the domain of phi.\n\n\n\n\n\n","category":"function"},{"location":"api/#AlgebraicInference.combine","page":"Library Reference","title":"AlgebraicInference.combine","text":"combine(ϕ₁::Valuation, ϕ₂::Valuation)\n\nPerform the combination phi_1 otimes phi_2.\n\n\n\n\n\n","category":"function"},{"location":"api/#AlgebraicInference.project","page":"Library Reference","title":"AlgebraicInference.project","text":"project(ϕ::Valuation, x)\n\nPerform the projection phi^downarrow x.\n\n\n\n\n\n","category":"function"},{"location":"api/#AlgebraicInference.inference_problem-Tuple{AbstractUWD, AbstractDict}","page":"Library Reference","title":"AlgebraicInference.inference_problem","text":"inference_problem(wd::UndirectedWiringDiagram, box_map::AbstractDict)\n\nSee inference_problem(wd::UndirectedWiringDiagram, boxes::AbstractVector).\n\n\n\n\n\n","category":"method"},{"location":"api/#AlgebraicInference.inference_problem-Tuple{AbstractUWD, AbstractVector}","page":"Library Reference","title":"AlgebraicInference.inference_problem","text":"inference_problem(wd::UndirectedWiringDiagram, boxes::AbstractVector)\n\nLet f be an operation in Cospan of the form\n\n    B xleftarrowmathttbox P xrightarrowmathttjunc J\n    xleftarrowmathttjunc Q\n\nand (b_1 dots b_n) a sequence of fillers for the boxes in f. Then inference_problem(wd, boxes) constructs a knowledge base phi_1 dots phi_n and query x subseteq J such that\n\n    (phi_1 otimes dots otimes phi_n)^downarrow x cong\n    F(f)(b_1 dots b_n)\n\nwhere F is the Cospan-algebra computed by oapply.\n\nThe operation f must satify must satisfy the following constraints:\n\nmathttjunc is injective.\nmathttimage(mathttjunc) subseteq mathttimage(mathttjunc)\nFor all x y in P, mathttbox(x) = mathttbox(y) and mathttjunc(x) = mathttjunc(y) implies that x = y. \n\n\n\n\n\n","category":"method"},{"location":"api/#Architectures","page":"Library Reference","title":"Architectures","text":"","category":"section"},{"location":"api/","page":"Library Reference","title":"Library Reference","text":"Architecture\narchitecture\nanswer_query\nanswer_query!","category":"page"},{"location":"api/#AlgebraicInference.Architecture","page":"Library Reference","title":"AlgebraicInference.Architecture","text":"Architecture{T₁, T₂} <: AbstractNode{T₁}\n\nA join tree (V E lambda D), along with a set of factors\n\n    left phi_i right_i in V\n\nand mailboxes\n\n    left left( mu_ i to mathttpa(i) mu_mathttpa(i) to i right) right_i in V\n\n\n\n\n\n","category":"type"},{"location":"api/#AlgebraicInference.architecture","page":"Library Reference","title":"AlgebraicInference.architecture","text":"architecture(kb::AbstractVector{<:Valuation{T}}, order) where T\n\nConstruct a covering join tree for the knowledge base kb using the variable elimination order order.\n\n\n\n\n\n","category":"function"},{"location":"api/#AlgebraicInference.answer_query","page":"Library Reference","title":"AlgebraicInference.answer_query","text":"answer_query(jt::Architecture{T₁, T₂}, query::Set{T₂}) where {T₁, T₂}\n\nAnswer a query.\n\nLet (V E lambda D) be a join tree with factors phi_i_i in V and x a query covered by (V E lambda D). Then answer_query(jt, query) solves the inference problem\n\nleft( bigotimes_i in V psi_i right)^downarrow x\n\n\n\n\n\n","category":"function"},{"location":"api/#AlgebraicInference.answer_query!","page":"Library Reference","title":"AlgebraicInference.answer_query!","text":"answer_query!(jt::Architecture{T₁, T₂}, query::Set{T₂}) where {T₁, T₂}\n\nAnswer a query, caching intermediate computations.\n\nLet (V E lambda D) be a join tree with factors phi_i_i in V and x be a query covered by (V E lambda D). Then answer_query!(jt, query) solves the inference problem\n\nleft( bigotimes_i in V psi_i right)^downarrow x\n\n\n\n\n\n","category":"function"},{"location":"api/#Graphs","page":"Library Reference","title":"Graphs","text":"","category":"section"},{"location":"api/","page":"Library Reference","title":"Library Reference","text":"primal_graph\nminfill!\nminwidth!","category":"page"},{"location":"api/#AlgebraicInference.primal_graph","page":"Library Reference","title":"AlgebraicInference.primal_graph","text":"primal_graph(kb::AbstractVector{<:Valuation{T}}) where T\n\nConstruct the primal graph of the knowledge base kb.\n\n\n\n\n\n","category":"function"},{"location":"api/#AlgebraicInference.minfill!","page":"Library Reference","title":"AlgebraicInference.minfill!","text":"minfill!(g::MetaGraph, query)\n\nCompute a vertex elimination order using the min-fill heuristic.\n\n\n\n\n\n","category":"function"},{"location":"api/#AlgebraicInference.minwidth!","page":"Library Reference","title":"AlgebraicInference.minwidth!","text":"minwidth!(g::MetaGraph, query)\n\nCompute a vertex elimination order using the min-width heuristic \n\n\n\n\n\n","category":"function"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"EditURL = \"https://github.com/samuelsonric/AlgebraicInference.jl/blob/master/docs/literate/regression.jl\"","category":"page"},{"location":"generated/regression/#Linear-Regression","page":"Linear Regression","title":"Linear Regression","text":"","category":"section"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"using AlgebraicInference\nusing Catlab, Catlab.Graphics, Catlab.Programs\nusing LinearAlgebra\nusing StatsPlots","category":"page"},{"location":"generated/regression/#Frequentist-Linear-Regression","page":"Linear Regression","title":"Frequentist Linear Regression","text":"","category":"section"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"Consider the Gauss-Markov linear model","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"    y = X beta + epsilon","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"where X is an n times m matrix, beta is an m times 1 vector, and epsilon is an n times 1 normally distributed random vector with mean mathbf0 and covariance W. If X has full column rank, then the best linear unbiased estimator for beta is the random vector","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"    hatbeta = X^+ (I - (Q W Q)^+ Q W)^mathsfT y","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"where X^+ is the Moore-Penrose psuedoinverse of X, and","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"Q = I - X X^+","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"References:","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"Albert, Arthur. \"The Gauss-Markov Theorem for Regression Models with Possibly Singular Covariances.\" SIAM Journal on Applied Mathematics, vol. 24, no. 2, 1973, pp. 182–87.","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"X = [\n    1 0\n    0 1\n    0 0\n]\n\nW = [\n    1 0 0\n    0 1 0\n    0 0 1\n]\n\ny = [\n    1\n    1\n    1\n]\n\nQ = I - X * pinv(X)\nβ̂ = pinv(X) * (I - pinv(Q * W * Q) * Q * W)' * y","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"To solve for hatbeta using AlgebraicInference.jl, we construct an undirected wiring diagram.","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"diagram = @relation (a₁, a₂) begin\n    X(a₁, a₂, b₁, b₂, b₃)\n    +(b₁, b₂, b₃, c₁, c₂, c₃, d₁, d₂, d₃)\n    ϵ(c₁, c₂, c₃)\n    y(d₁, d₂, d₃)\nend\n\nto_graphviz(diagram; box_labels=:name, implicit_junctions=true)","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"Then we assign values to the boxes in diagram and compute the result.","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"P = [\n    1 0 0 1 0 0\n    0 1 0 0 1 0\n    0 0 1 0 0 1\n]\n\nbox_map = Dict(\n    :X => kernel(X),\n    :+ => kernel(P),\n    :ϵ => normal(W),\n    :y => normal(y))\n\nβ̂ = mean(oapply(diagram, box_map))","category":"page"},{"location":"generated/regression/#Bayesian-Linear-Regression","page":"Linear Regression","title":"Bayesian Linear Regression","text":"","category":"section"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"Let rho = mathcalN(m V) be our prior belief about beta. Then our posterior belief hatrho is a bivariate normal distribution with mean","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"  hatm = m - V X^mathsfT (X V X + W)^+ (X m - y)","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"and covariance","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"  hatV = V - V X^mathsfT (X V X + W)^+ X V","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"V = [\n    1 0\n    0 1\n]\n\nm = [\n    0\n    0\n]\n\nm̂ = m - V * X' * pinv(X * V * X' + W) * (X * m - y)","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"V̂ = V - V * X' * pinv(X * V * X' + W) * X * V","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"To solve for hatrho using AlgebraicInference.jl, we construct an undirected wiring diagram.","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"diagram = @relation (a₁, a₂) begin\n    ρ(a₁, a₂)\n    X(a₁, a₂, b₁, b₂, b₃)\n    +(b₁, b₂, b₃, c₁, c₂, c₃, d₁, d₂, d₃)\n    ϵ(c₁, c₂, c₃)\n    y(d₁, d₂, d₃)\nend\n\nto_graphviz(diagram; box_labels=:name, implicit_junctions=true)","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"Then we assign values to the boxes in diagram and compute the result.","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"box_map = Dict(\n    :ρ => normal(V, m),\n    :X => kernel(X),\n    :+ => kernel(P),\n    :ϵ => normal(W),\n    :y => normal(y))\n\nm̂ = mean(oapply(diagram, box_map))","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"V̂ = cov(oapply(diagram, box_map))","category":"page"},{"location":"generated/regression/","page":"Linear Regression","title":"Linear Regression","text":"covellipse!(m, V, aspect_ratio=:equal, label=\"prior\")\ncovellipse!(m̂, V̂, aspect_ratio=:equal, label=\"posterior\")","category":"page"},{"location":"generated/kalman/","page":"Kalman Filter","title":"Kalman Filter","text":"EditURL = \"https://github.com/samuelsonric/AlgebraicInference.jl/blob/master/docs/literate/kalman.jl\"","category":"page"},{"location":"generated/kalman/#Kalman-Filter","page":"Kalman Filter","title":"Kalman Filter","text":"","category":"section"},{"location":"generated/kalman/","page":"Kalman Filter","title":"Kalman Filter","text":"using AlgebraicInference\nusing BenchmarkTools\nusing Catlab, Catlab.Graphics, Catlab.Programs, Catlab.WiringDiagrams\nusing Catlab.WiringDiagrams.MonoidalUndirectedWiringDiagrams: UntypedHypergraphDiagram\nusing Distributions\nusing GraphPlot\nusing LinearAlgebra\nusing Random","category":"page"},{"location":"generated/kalman/","page":"Kalman Filter","title":"Kalman Filter","text":"A Kalman filter with n steps is a probability distribution over states (s_1 dots s_n) and measurements (z_1 dots z_n) determined by the equations","category":"page"},{"location":"generated/kalman/","page":"Kalman Filter","title":"Kalman Filter","text":"    s_i+1 mid s_i sim mathcalN(As_i P)","category":"page"},{"location":"generated/kalman/","page":"Kalman Filter","title":"Kalman Filter","text":"and","category":"page"},{"location":"generated/kalman/","page":"Kalman Filter","title":"Kalman Filter","text":"    z_i mid s_i sim mathcalN(Bs_i Q)","category":"page"},{"location":"generated/kalman/","page":"Kalman Filter","title":"Kalman Filter","text":"θ = π / 15\n\nA = [\n    cos(θ) -sin(θ)\n    sin(θ) cos(θ)\n]\n\nB = [\n    1.3 0.0\n    0.0 0.7\n]\nP = [\n    0.05 0.0\n    0.0 0.05\n]\n\nQ = [\n    10.0 0.0\n    0.0 10.0\n]\n\nfunction generate_data(n; seed=42)\n    Random.seed!(seed)\n    x = zeros(2)\n    data = Vector{Float64}[]\n\n    for i in 1:n\n        x = rand(MvNormal(A * x, P))\n        push!(data, rand(MvNormal(B * x, Q)))\n    end\n\n    data\nend;\nnothing #hide","category":"page"},{"location":"generated/kalman/","page":"Kalman Filter","title":"Kalman Filter","text":"The filtering problem involves predicting the value of the state s_n given observations of (z_1 dots z_n). The function kalman constructs a wiring diagram that represents the filtering problem.","category":"page"},{"location":"generated/kalman/","page":"Kalman Filter","title":"Kalman Filter","text":"function kalman_step(i)\n    kf = UntypedHypergraphDiagram{Symbol}(2)\n    add_box!(kf, 2; name=:state)\n    add_box!(kf, 4; name=:predict)\n    add_box!(kf, 4; name=:measure)\n    add_box!(kf, 2; name=Symbol(\"z$i\"))\n\n    add_wires!(kf, [\n        (0, 1) => (2, 3),\n        (0, 2) => (2, 4),\n        (1, 1) => (2, 1),\n        (1, 1) => (3, 1),\n        (1, 2) => (2, 2),\n        (1, 2) => (3, 2),\n        (3, 3) => (4, 1),\n        (3, 4) => (4, 2)])\n\n    kf\nend\n\nfunction kalman(n)\n    reduce((kf, i) -> ocompose(kalman_step(i), 1, kf), 2:n; init=kalman_step(1))\nend\n\nto_graphviz(kalman(5), box_labels=:name; implicit_junctions=true)","category":"page"},{"location":"generated/kalman/","page":"Kalman Filter","title":"Kalman Filter","text":"We generate 100 points of data and solve the filtering problem.","category":"page"},{"location":"generated/kalman/","page":"Kalman Filter","title":"Kalman Filter","text":"n = 100; kf = kalman(n); data = generate_data(n)\n\nbox_map = Dict(\n    :state => normal(100I(2)),\n    :predict => kernel(P, A),\n    :measure => kernel(Q, B))\n\nfor i in 1:n\n    box_map[Symbol(\"z$i\")] = normal(data[i])\nend\n\nmean(oapply(kf, box_map))","category":"page"},{"location":"generated/kalman/","page":"Kalman Filter","title":"Kalman Filter","text":"@benchmark oapply(kf, box_map)","category":"page"},{"location":"generated/kalman/","page":"Kalman Filter","title":"Kalman Filter","text":"Although we can solve the filtering problem using oapply, it is more efficient to use variable elimination. The function inference_problem turns the wiring diagram kf into an undirected graphical model.","category":"page"},{"location":"generated/kalman/","page":"Kalman Filter","title":"Kalman Filter","text":"kb, query = inference_problem(kf, box_map)\npg = primal_graph(kb)\n\ngplot(pg)","category":"page"},{"location":"generated/kalman/","page":"Kalman Filter","title":"Kalman Filter","text":"We compute a variable elimination order using the \"min-fill\" heuristic.","category":"page"},{"location":"generated/kalman/","page":"Kalman Filter","title":"Kalman Filter","text":"order = minfill!(pg, query)\njt = architecture(kb, order)\n\nmean(answer_query(jt, query).box)","category":"page"},{"location":"generated/kalman/","page":"Kalman Filter","title":"Kalman Filter","text":"@benchmark answer_query(jt, query)","category":"page"},{"location":"#AlgebraicInference.jl","page":"AlgebraicInference.jl","title":"AlgebraicInference.jl","text":"","category":"section"},{"location":"","page":"AlgebraicInference.jl","title":"AlgebraicInference.jl","text":"AlgebraicInference.jl is a library for compositional Bayesian inference. It builds on Catlab.jl.","category":"page"}]
}
